{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies and Initial Plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model Setup (Ollama + DeepSeek)\n",
    "\n",
    "2. Data Preparation (Knowledge Source)\n",
    "\n",
    "3. Chunk & Embed Data\n",
    "\n",
    "4. Build Vector DB\n",
    "\n",
    "5. Query Pipeline (RAG Flow)\n",
    "\n",
    "6. Streamlit Setup (UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in f:\\chatbot\\chatbot_env\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-community in f:\\chatbot\\chatbot_env\\lib\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-core in f:\\chatbot\\chatbot_env\\lib\\site-packages (0.3.51)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: PyYAML>=5.3 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: requests<3,>=2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (2.1.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (3.11.16)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-core) (4.13.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: idna in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: certifi in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: anyio in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: greenlet>=1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\Chatbot\\chatbot_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup (Ollama + DeepSeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Function to get response from Ollama DeepSeek 1.5B\n",
    "def ollama_response(query: str) -> str:\n",
    "    response = ollama.chat(model=\"llama3.2:3b\", messages=[{\"role\": \"user\", \"content\": query}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Testing the setup with a simple query\n",
    "print(ollama_response(\"Hello, how are you?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did the AI program go to therapy?\n",
      "\n",
      "Because it was struggling to process its emotions! (get it?)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Instantiate the model\n",
    "llm = Ollama(model=\"llama3.2:3b\")\n",
    "\n",
    "# Example prompt setup\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about AI.\")\n",
    "\n",
    "# Run the model with the prompt\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Knowledge Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome to **Upskill** â€” your gateway to a future in Software Quality Assurance (SQA)!\\n\\n**Why Choose Upskill?**\\nAt Upskill, we donâ€™t just teach â€” we transform beginners into QA professionals ready for the global tech industry. Our SQA course is crafted by industry experts to meet real-world needs.\\n\\n---\\n\\n**What Youâ€™ll Learn in the SQA Course:**\\nâœ… Fundamentals of Software Testing  \\nâœ… Test Planning & Case Design  \\nâœ… Manual & Automated Testing Techniques  \\nâœ… Tools: Selenium, JIRA, Postman, Git  \\nâœ… Bug Reporting & Defect Life Cycle  \\nâœ… Agile & DevOps Essentials  \\nâœ… Resume Building + Mock Interviews  \\n\\n---\\n\\n**Who is this for?**\\n- Fresh graduates looking to enter tech  \\n- Career switchers from non-IT backgrounds  \\n- Professionals seeking QA upskilling  \\nNo coding background? No problem! Weâ€™ll guide you step-by-step.\\n\\n---\\n\\n**100% Online | Live Instructor-Led Classes | Hands-On Projects**\\n\\n**Real Projects + Internship Opportunity**\\nWork on real-world testing projects. Stand out with experience that hiring managers notice.\\n\\n---\\n\\n**Why SQA is a Great Career Path?**\\n- High demand globally  \\n- Great salary growth  \\n- Opportunities to work in agile teams  \\n- A critical role in product quality & user satisfaction  \\n\\n---\\n\\n**Contact Upskill Today**\\n- Website: www.upskill.com  \\n- WhatsApp: +8801XXXXXXXXX  \\n- Email: info@upskill.com\\n-  CEO : Rasel Alam\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "#Create .txt, .pdf, or .md file (example: docs/my_faqs.txt)\n",
    "\n",
    "#Keep clean, structured knowledge (questions, answers, docs)\n",
    "\n",
    "import os\n",
    "\n",
    "def load_documents_from_folder(folder_path: str):\n",
    "    documents = []\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Check for .txt files\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "    # If no .txt file exists, create one\n",
    "    if not txt_files:\n",
    "        default_path = os.path.join(folder_path, \"knowledge.txt\")\n",
    "        with open(default_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"This is a default knowledge base. Please add more .txt files.\")\n",
    "        txt_files.append(\"knowledge.txt\")\n",
    "\n",
    "    # Load all .txt files\n",
    "    for filename in txt_files:\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            documents.append(file.read())\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"./knowledge_source\"\n",
    "documents = load_documents_from_folder(folder_path)\n",
    "print(documents[:1])  # print the first document\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk & Embed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… knowledge.txt loaded successfully.\n",
      "First chunk: ['Welcome to **Upskill** â€” your gateway to a future in Software Quality Assurance (SQA)!\\n\\n**Why Choose Upskill?**\\nAt Upskill, we donâ€™t just teach â€” we transform beginners into QA professionals ready for the global tech industry. Our SQA course is crafted by industry experts to meet real-world needs.\\n\\n---']\n"
     ]
    }
   ],
   "source": [
    "#Use LangChain to:\n",
    "\n",
    "#Load your documents\n",
    "\n",
    "#Split into smaller chunks\n",
    "\n",
    "#Embed with HuggingFaceEmbeddings\n",
    "\n",
    "#Store into Chroma vector DB\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Use a small, fast model for local embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Chunking function\n",
    "def chunk_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(text_splitter.split_text(doc))\n",
    "    return chunks\n",
    "\n",
    "# Load knowledge from file (assuming knowledge.txt exists)\n",
    "def load_knowledge_file():\n",
    "    file_path = \"knowledge_source/knowledge.txt\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "            print(\"âœ… knowledge.txt loaded successfully.\")\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error reading file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Load knowledge base\n",
    "knowledge_text = load_knowledge_file()\n",
    "\n",
    "# If knowledge is loaded, chunk it\n",
    "if knowledge_text:\n",
    "    documents = [knowledge_text]  # Wrap the text into a list to pass to chunk_documents\n",
    "    chunks = chunk_documents(documents)\n",
    "    print(f\"First chunk: {chunks[:1]}\")  # Show the first chunk\n",
    "else:\n",
    "    print(\"âš ï¸ No knowledge to chunk.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector DB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks added to vector DB!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or load the collection for storing the vectors\n",
    "persist_directory = \"./chromadb_persist\"\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.mkdir(persist_directory)\n",
    "\n",
    "# Create collection in Chroma\n",
    "collection = client.get_or_create_collection(name=\"chatbot_collection\")\n",
    "\n",
    "\n",
    "# Create the Chroma vector store\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "# Add chunks to Chroma DB\n",
    "vectorstore.add_texts(chunks)\n",
    "print(\"Chunks added to vector DB!\")\n",
    "\n",
    "\n",
    "\n",
    "# working on this to find solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Pipeline (RAG Flow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "---\n",
      "\n",
      "ðŸŽ¯ **Why SQA is a Great Career Path?**\n",
      "- High demand globally  \n",
      "- Great salary growth  \n",
      "- Opportunities to work in agile teams  \n",
      "- A critical role in product quality & user satisfaction  \n",
      "\n",
      "---\n",
      "\n",
      "ðŸ“ž **Contact Upskill Today**\n",
      "- Website: www.upskill.com  \n",
      "- WhatsApp: +8801XXXXXXXXX  \n",
      "- Email: info@upskill.com\n",
      "\n",
      "ðŸ§  Learn QA. Get Certified. Launch Your Career â€” with Upskill.\n",
      "\n",
      "---\n",
      "\n",
      "ðŸŽ¯ **Why SQA is a Great Career Path?**\n",
      "- High demand globally  \n",
      "- Great salary growth  \n",
      "- Opportunities to work in agile teams  \n",
      "- A critical role in product quality & user satisfaction  \n",
      "\n",
      "---\n",
      "\n",
      "ðŸ“ž **Contact Upskill Today**\n",
      "- Website: www.upskill.com  \n",
      "- WhatsApp: +8801XXXXXXXXX  \n",
      "- Email: info@upskill.com\n",
      "\n",
      "ðŸ§  Learn QA. Get Certified. Launch Your Career â€” with Upskill.\n",
      "\n",
      "---\n",
      "\n",
      "ðŸŽ¯ **Why SQA is a Great Career Path?**\n",
      "- High demand globally  \n",
      "- Great salary growth  \n",
      "- Opportunities to work in agile teams  \n",
      "- A critical role in product quality & user satisfaction  \n",
      "\n",
      "---\n",
      "\n",
      "ðŸ“ž **Contact Upskill Today**\n",
      "- Website: www.upskill.com  \n",
      "- WhatsApp: +8801XXXXXXXXX  \n",
      "- Email: info@upskill.com\n",
      "\n",
      "ðŸ§  Learn QA. Get Certified. Launch Your Career â€” with Upskill.\n",
      "\n",
      "---\n",
      "\n",
      "ðŸŽ¯ **Why SQA is a Great Career Path?**\n",
      "- High demand globally  \n",
      "- Great salary growth  \n",
      "- Opportunities to work in agile teams  \n",
      "- A critical role in product quality & user satisfaction  \n",
      "\n",
      "---\n",
      "\n",
      "ðŸ“ž **Contact Upskill Today**\n",
      "- Website: www.upskill.com  \n",
      "- WhatsApp: +8801XXXXXXXXX  \n",
      "- Email: info@upskill.com\n",
      "CEO - Rasel Alam\n",
      "\n",
      "ðŸ§  Learn QA. Get Certified. Launch Your Career â€” with Upskill.\n",
      "\n",
      "Question: why sqa is a great careerpath?\n",
      "Helpful Answer: historical zee nfl macon 737 impress lucas bbc hollywoodsport thomson calgary crore toy puneurer southampton chilerie med\n"
     ]
    }
   ],
   "source": [
    "#Accept user question\n",
    "\n",
    "#Convert to embedding\n",
    "\n",
    "#Search top-k similar chunks from Chroma\n",
    "\n",
    "#Concatenate chunks + question\n",
    "\n",
    "#Send to OllamaLLM via LangChain\n",
    "\n",
    "#Get DeepSeek response\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the Hugging Face model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Replace with your desired model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use the pipeline to create a conversational model\n",
    "huggingface_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize HuggingFacePipeline directly without BaseChatModel\n",
    "llm = HuggingFacePipeline(pipeline=huggingface_pipeline)\n",
    "\n",
    "# Initialize the retrieval QA system with the Hugging Face model\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())\n",
    "\n",
    "# Function to process query using RAG flow\n",
    "def query_pipeline(query: str) -> str:\n",
    "    # Initialize an empty chat history\n",
    "    chat_history = []\n",
    "    \n",
    "    # Create a dictionary with the 'question' and 'chat_history' keys\n",
    "    input_data = {\"question\": query, \"chat_history\": chat_history}\n",
    "    \n",
    "    # Run the query through the QA chain\n",
    "    response = qa_chain.run(input_data)\n",
    "    return response\n",
    "\n",
    "# Test the query pipeline\n",
    "print(query_pipeline(\"why sqa is a great careerpath?\"))  # Replace with your question\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know who the CEO of Upskill is. The provided context only mentions the mission and benefits of the Upskill program, but does not include any information about the organization's leadership or specific individuals in key roles.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load the Ollama LLM (llama3.2)\n",
    "llm = Ollama(model=\"llama3.2:3b\")\n",
    "\n",
    "# Use your already setup Chroma vectorstore\n",
    "# Assume variable name is `vectorstore`\n",
    "\n",
    "# Create the Retrieval Chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "#  Query Function\n",
    "def query_pipeline(query: str, chat_history: list = []) -> str:\n",
    "    input_data = {\"question\": query, \"chat_history\": chat_history}\n",
    "    response = qa_chain.invoke(input_data)\n",
    "    return response[\"answer\"]\n",
    "\n",
    "# Test it\n",
    "print(query_pipeline(\"Who is the CEO of Upskill?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# === Load Ollama Model ===\n",
    "# Make sure your model (like deepseek:1.5b) is running via `ollama run deepseek`\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# === Build Retrieval-Augmented Generation Chain ===\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()  # Make sure vectorstore is initialized with your docs\n",
    ")\n",
    "\n",
    "# === Query Function with Chat History ===\n",
    "chat_history = []\n",
    "\n",
    "def query_pipeline(query: str) -> str:\n",
    "    global chat_history\n",
    "    response = qa_chain.invoke({\n",
    "        \"question\": query,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    # Update chat history for conversational memory\n",
    "    chat_history.append((query, response[\"answer\"]))\n",
    "    return response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out what \"Upskill\" is based on the given context. Let me read through it again carefully. \n",
      "\n",
      "The first line says, \"Welcome to Upskill â€” your gateway to a future in Software Quality Assurance (SQA)!\" That immediately tells me that Upskill is an educational resource focused on SQA. It's part of a series, but all seem to lead up to the same main point.\n",
      "\n",
      "Then it mentions why choose Upskill: they don't just teach; they transform beginners into QA professionals ready for the global tech industry. So, clearly,Upskill provides not only basic training but helps learners transition smoothly to becoming a QA professional in an IT field. They're preparing people for real-world demands in the tech industry.\n",
      "\n",
      "I also see that it's crafted by industry experts, which suggests it's based on real-world expertise and practical needs. The course is designed to meet the real-world needs of SQA professionals, meaning they've considered what skills are most important for transitioning into this field.\n",
      "\n",
      "Looking through each passage again doesn't give me any new information beyond what's already there because all entries repeat the same opening line about Upskill being a gateway and its focus on SQA. So, I don't see additional details that can help me understand more about what Upskill does or how it differs from other resources.\n",
      "\n",
      "Therefore, based on the context provided, the best answer is that Upskill is an educational resource dedicated to teaching Software Quality Assurance with the goal of helping beginners become proficient professionals in a global tech environment.\n",
      "</think>\n",
      "\n",
      "Upskill is an educational resource focused on Software Quality Assurance (SQA), providing comprehensive training to help individuals transition smoothly into this field. It emphasizes transforming beginners into QA professionals ready for the demands of the global tech industry, using expertise and practical needs as guiding principles.\n"
     ]
    }
   ],
   "source": [
    "print(query_pipeline(\"Tell me about Upskill\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit Setup (UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
