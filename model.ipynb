{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies and Initial Plan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model Setup (Ollama + DeepSeek)\n",
    "\n",
    "2. Data Preparation (Knowledge Source)\n",
    "\n",
    "3. Chunk & Embed Data\n",
    "\n",
    "4. Build Vector DB\n",
    "\n",
    "5. Query Pipeline (RAG Flow)\n",
    "\n",
    "6. Streamlit Setup (UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "Collecting langchain-core\n",
      "  Using cached langchain_core-0.3.51-py3-none-any.whl (423 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Collecting langsmith<0.4,>=0.1.17\n",
      "  Using cached langsmith-0.3.31-py3-none-any.whl (358 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached sqlalchemy-2.0.40-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain) (2.11.3)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (2.1.3)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.11.16-cp310-cp310-win_amd64.whl (442 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langchain-core) (4.13.2)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-win_amd64.whl (51 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.3-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.1-cp310-cp310-win_amd64.whl (45 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.19.0-cp310-cp310-win_amd64.whl (92 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: certifi in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
      "Requirement already satisfied: anyio in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: idna in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Collecting greenlet>=1\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-win_amd64.whl (298 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in f:\\chatbot\\chatbot_env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Installing collected packages: langsmith, jsonpatch, propcache, mypy-extensions, multidict, langchain-core, greenlet, frozenlist, yarl, typing-inspect, SQLAlchemy, python-dotenv, marshmallow, langchain-text-splitters, async-timeout, aiosignal, aiohappyeyeballs, pydantic-settings, langchain, httpx-sse, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.40 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 httpx-sse-0.4.0 jsonpatch-1.33 langchain-0.3.23 langchain-community-0.3.21 langchain-core-0.3.51 langchain-text-splitters-0.3.8 langsmith-0.3.31 marshmallow-3.26.1 multidict-6.4.3 mypy-extensions-1.0.0 propcache-0.3.1 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0 yarl-1.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'F:\\Chatbot\\chatbot_env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-core\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup (Ollama + DeepSeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model \"deepseek-1.5b\" not found, try pulling it first (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Testing the setup with a simple query\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mollama_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, how are you?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mollama_response\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mollama_response\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-1.5b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mf:\\Chatbot\\chatbot_env\\lib\\site-packages\\ollama\\_client.py:333\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Chatbot\\chatbot_env\\lib\\site-packages\\ollama\\_client.py:178\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, cls, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpart)\n\u001b[0;32m    176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_raw(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mf:\\Chatbot\\chatbot_env\\lib\\site-packages\\ollama\\_client.py:122\u001b[0m, in \u001b[0;36mClient._request_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mConnectError:\n\u001b[0;32m    124\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mResponseError\u001b[0m: model \"deepseek-1.5b\" not found, try pulling it first (status code: 404)"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Function to get response from Ollama DeepSeek 1.5B\n",
    "def ollama_response(query: str) -> str:\n",
    "    response = ollama.chat(model=\"deepseek-1.5b\", messages=[{\"role\": \"user\", \"content\": query}])\n",
    "    return response['text']\n",
    "\n",
    "# Testing the setup with a simple query\n",
    "print(ollama_response(\"Hello, how are you?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to come up with a joke that's specifically about AI. Hmm, where do I start? Well, the user already gave a joke about the future of machines, which seems like a good foundation. Maybe I can build on that idea or find something new.\n",
      "\n",
      "First, let me think about what's funny when you're talking about AI. It often involves comparing it to other technologies, maybe even the human brain. But wait, I should remember that AI isn't exactly human, but people often joke about its capabilities and how it could surpass human abilities in certain areas.\n",
      "\n",
      "Maybe I can take a classic pun or an old joke and twist it a bit to include some tech jargon related to AI. Let me recall some common phrases with AI. Oh, \"I'll never know what you've been through\" comes to mind. It's about not having personal experiences but talking about them hypothetically.\n",
      "\n",
      "But how does that tie into the joke? Well, perhaps comparing that line to a machine. The user did a good job there. Maybe I can add something more technical or humorous without making it too dry.\n",
      "\n",
      "What other puns could relate to AI? \"I'm not sure,\" is pretty common but a bit clich√©. Maybe find something that's less obvious but still funny. Or perhaps use an acronym related to AI, like Q&A, but that might be stretching the joke.\n",
      "\n",
      "Wait, maybe think about how AI can perform tasks beyond human capacity. Like, it can analyze data or create models automatically. That sounds interesting and a bit technical.\n",
      "\n",
      "So putting that together: \"I'm not sure if I'll ever know what you've been through.\" Then, comparing it to a machine because, you know, it's also doing something without being aware of its limitations. Or maybe even adding an emoji to make it more visual.\n",
      "\n",
      "Let me see how that works. It starts with the classic line about not knowing personal experiences but talking hypothetically. Then adds that part about comparing it to a machine since AI can perform tasks beyond human understanding, which ties back to the joke and makes it sound like a real statement from someone in tech.\n",
      "\n",
      "I think that should work. It's funny because it combines two unrelated ideas, making it more interesting than just repeating one. Plus, adding some personality with a smiley emoji keeps it light.\n",
      "</think>\n",
      "\n",
      "**Joke:**\n",
      "\n",
      "\"I'm not sure if I'll ever know what you've been through. But here's the thing‚Äîlike a machine. It can do things that even the best humans might find hard to understand.\"\n",
      "\n",
      "(üòä)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Instantiate the model\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# Example prompt setup\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about AI.\")\n",
    "\n",
    "# Run the model with the prompt\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation (Knowledge Source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a default knowledge base. Please add more .txt files.']\n"
     ]
    }
   ],
   "source": [
    "#Create .txt, .pdf, or .md file (example: docs/my_faqs.txt)\n",
    "\n",
    "#Keep clean, structured knowledge (questions, answers, docs)\n",
    "\n",
    "import os\n",
    "\n",
    "def load_documents_from_folder(folder_path: str):\n",
    "    documents = []\n",
    "\n",
    "    # Ensure folder exists\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Check for .txt files\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "    # If no .txt file exists, create one\n",
    "    if not txt_files:\n",
    "        default_path = os.path.join(folder_path, \"knowledge.txt\")\n",
    "        with open(default_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"This is a default knowledge base. Please add more .txt files.\")\n",
    "        txt_files.append(\"knowledge.txt\")\n",
    "\n",
    "    # Load all .txt files\n",
    "    for filename in txt_files:\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            documents.append(file.read())\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"./knowledge_source\"\n",
    "documents = load_documents_from_folder(folder_path)\n",
    "print(documents[:1])  # print the first document\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk & Embed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ knowledge.txt loaded successfully.\n",
      "First chunk: ['Welcome to **Upskill** ‚Äî your gateway to a future in Software Quality Assurance (SQA)!\\n\\nüöÄ **Why Choose Upskill?**\\nAt Upskill, we don‚Äôt just teach ‚Äî we transform beginners into QA professionals ready for the global tech industry. Our SQA course is crafted by industry experts to meet real-world needs.\\n\\n---']\n"
     ]
    }
   ],
   "source": [
    "#Use LangChain to:\n",
    "\n",
    "#Load your documents\n",
    "\n",
    "#Split into smaller chunks\n",
    "\n",
    "#Embed with HuggingFaceEmbeddings\n",
    "\n",
    "#Store into Chroma vector DB\n",
    "\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Use a small, fast model for local embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Chunking function\n",
    "def chunk_documents(documents, chunk_size=500, chunk_overlap=50):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(text_splitter.split_text(doc))\n",
    "    return chunks\n",
    "\n",
    "# Load knowledge from file (assuming knowledge.txt exists)\n",
    "def load_knowledge_file():\n",
    "    file_path = \"knowledge_source/knowledge.txt\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "            print(\"‚úÖ knowledge.txt loaded successfully.\")\n",
    "            return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Load knowledge base\n",
    "knowledge_text = load_knowledge_file()\n",
    "\n",
    "# If knowledge is loaded, chunk it\n",
    "if knowledge_text:\n",
    "    documents = [knowledge_text]  # Wrap the text into a list to pass to chunk_documents\n",
    "    chunks = chunk_documents(documents)\n",
    "    print(f\"First chunk: {chunks[:1]}\")  # Show the first chunk\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No knowledge to chunk.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector DB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faisal\\AppData\\Local\\Temp\\ipykernel_3656\\2869819951.py:16: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks added to vector DB!\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or load the collection for storing the vectors\n",
    "persist_directory = \"./chromadb_persist\"\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.mkdir(persist_directory)\n",
    "\n",
    "# Create collection in Chroma\n",
    "collection = client.create_collection(name=\"chatbot_collection\")\n",
    "\n",
    "# Create the Chroma vector store\n",
    "vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "# Add chunks to Chroma DB\n",
    "vectorstore.add_texts(chunks)\n",
    "print(\"Chunks added to vector DB!\")\n",
    "\n",
    "\n",
    "\n",
    "# working on this to find solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Pipeline (RAG Flow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Welcome to **Upskill** ‚Äî your gateway to a future in Software Quality Assurance (SQA)!\n",
      "\n",
      "üöÄ **Why Choose Upskill?**\n",
      "At Upskill, we don‚Äôt just teach ‚Äî we transform beginners into QA professionals ready for the global tech industry. Our SQA course is crafted by industry experts to meet real-world needs.\n",
      "\n",
      "---\n",
      "\n",
      "---\n",
      "\n",
      "üéØ **Why SQA is a Great Career Path?**\n",
      "- High demand globally  \n",
      "- Great salary growth  \n",
      "- Opportunities to work in agile teams  \n",
      "- A critical role in product quality & user satisfaction  \n",
      "\n",
      "---\n",
      "\n",
      "üìû **Contact Upskill Today**\n",
      "- Website: www.upskill.com  \n",
      "- WhatsApp: +8801XXXXXXXXX  \n",
      "- Email: info@upskill.com\n",
      "\n",
      "üß† Learn QA. Get Certified. Launch Your Career ‚Äî with Upskill.\n",
      "\n",
      "---\n",
      "\n",
      "üí° **Who is this for?**\n",
      "- Fresh graduates looking to enter tech  \n",
      "- Career switchers from non-IT backgrounds  \n",
      "- Professionals seeking QA upskilling  \n",
      "No coding background? No problem! We‚Äôll guide you step-by-step.\n",
      "\n",
      "---\n",
      "\n",
      "üåê **100% Online | Live Instructor-Led Classes | Hands-On Projects**\n",
      "\n",
      "üíº **Real Projects + Internship Opportunity**\n",
      "Work on real-world testing projects. Stand out with experience that hiring managers notice.\n",
      "\n",
      "---\n",
      "\n",
      "---\n",
      "\n",
      "üìö **What You‚Äôll Learn in the SQA Course:**\n",
      "‚úÖ Fundamentals of Software Testing  \n",
      "‚úÖ Test Planning & Case Design  \n",
      "‚úÖ Manual & Automated Testing Techniques  \n",
      "‚úÖ Tools: Selenium, JIRA, Postman, Git  \n",
      "‚úÖ Bug Reporting & Defect Life Cycle  \n",
      "‚úÖ Agile & DevOps Essentials  \n",
      "‚úÖ Resume Building + Mock Interviews  \n",
      "\n",
      "---\n",
      "\n",
      "Question: Tell me about Upskill.\n",
      "Helpful Answer: compares deep cookies degrees degrees codex european republican cookies degrees definition vegas pac curling worth authoritative highlights highlights highlights\n"
     ]
    }
   ],
   "source": [
    "#Accept user question\n",
    "\n",
    "#Convert to embedding\n",
    "\n",
    "#Search top-k similar chunks from Chroma\n",
    "\n",
    "#Concatenate chunks + question\n",
    "\n",
    "#Send to OllamaLLM via LangChain\n",
    "\n",
    "#Get DeepSeek response\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the Hugging Face model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Replace with your desired model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use the pipeline to create a conversational model\n",
    "huggingface_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize HuggingFacePipeline directly without BaseChatModel\n",
    "llm = HuggingFacePipeline(pipeline=huggingface_pipeline)\n",
    "\n",
    "# Initialize the retrieval QA system with the Hugging Face model\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever())\n",
    "\n",
    "# Function to process query using RAG flow\n",
    "def query_pipeline(query: str) -> str:\n",
    "    # Initialize an empty chat history\n",
    "    chat_history = []\n",
    "    \n",
    "    # Create a dictionary with the 'question' and 'chat_history' keys\n",
    "    input_data = {\"question\": query, \"chat_history\": chat_history}\n",
    "    \n",
    "    # Run the query through the QA chain\n",
    "    response = qa_chain.run(input_data)\n",
    "    return response\n",
    "\n",
    "# Test the query pipeline\n",
    "print(query_pipeline(\"Tell me about Upskill.\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# === Load Ollama Model ===\n",
    "# Make sure your model (like deepseek:1.5b) is running via `ollama run deepseek`\n",
    "llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# === Build Retrieval-Augmented Generation Chain ===\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()  # Make sure vectorstore is initialized with your docs\n",
    ")\n",
    "\n",
    "# === Query Function with Chat History ===\n",
    "chat_history = []\n",
    "\n",
    "def query_pipeline(query: str) -> str:\n",
    "    global chat_history\n",
    "    response = qa_chain.invoke({\n",
    "        \"question\": query,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    # Update chat history for conversational memory\n",
    "    chat_history.append((query, response[\"answer\"]))\n",
    "    return response[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, I'm trying to figure out what \"Upskill\" is based on the provided context. Let's go through each part of the information given step by step.\n",
      "\n",
      "First, there's a welcome message that introduces Upskill as a gateway to software quality assurance (SQA). That tells me it's an educational or training platform focused on helping people gain expertise in SQA fields like testing and software development.\n",
      "\n",
      "Next, under \"Why Choose Upskill?\", it explains that the course is crafted by industry experts to meet real-world needs. This suggests that Upskill's content is practical and relevant to actual job scenarios. They emphasize transforming beginners into QA professionals ready for the tech industry, which implies that they're preparing individuals to work in a professional environment.\n",
      "\n",
      "The \"Why SQA is a Great Career Path?\" section highlights high demand globally, salary growth, agile team opportunities, and critical role in product quality and user satisfaction. This shows that Upskill isn't just about learning but also about the long-term benefits of such expertise, which are significant for career advancement.\n",
      "\n",
      "Looking at the details provided, Upskill is an online platform with live instructor-led classes and hands-on projects. Real-world projects and internships help individuals build their skills. There's no requirement for coding background, making it accessible to a wide range of people.\n",
      "\n",
      "The course covers fundamental testing topics like fundamentals, test planning, case design, manual and automated techniques, tools such as Selenium, JIRA, Postman, Git, bug reporting cycles, and an overview of Agile/DevOps essentials. They also focus on resume building and mock interviews. These subjects are essential for a career in QA.\n",
      "\n",
      "Putting it all together, Upskill appears to be a comprehensive training program aimed at equipping individuals with the necessary skills and knowledge in SQA through practical, hands-on projects and real-world applications. It's designed to help people gain certified expertise quickly.\n",
      "</think>\n",
      "\n",
      "Upskill is an educational platform dedicated to teaching software quality assurance (SQA) professionals. It offers a structured course that covers fundamental testing topics like fundamentals, test planning, case design, manual/automated techniques, tools, bug reporting/cycle, and Agile/DevOps essentials. The program emphasizes practical learning through real-world projects and internships to prepare individuals for the job market. No coding background is required, making it accessible to a wide range of people. Upskill aims to equip individuals with certified SQA expertise quickly.\n"
     ]
    }
   ],
   "source": [
    "print(query_pipeline(\"Tell me about Upskill\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit Setup (UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
